* 2016.05.18 - calculating compute capability for GPGPU ...
             - passing more internal vars to customized autotuner

* 2016.05.17 - adding '--no_vars' to skip default -D variables and hence use default vars from the sources

* 2016.05.16 - adding 'program.behavior' module to crowd-benchmark/crowd-tune program behavior
               across diverse data sets and hardware

* 2016.05.13 - moving platform.gpgpu here from ck-env repository to prepare for CUDA/OpenCL crowd-tuning

* 2016.05.06 - adding dummy JULIA language meta info (optimizations), after MIT visit

* 2016.04.28 - major update to packages and environment to automate detection of already 
               installed software (for Windows, Linux and partially Android).

             - adding compiler_tags to force compiler type (llvm,icc,gcc,microsoft,ctuning-cc,...)

* 2016.04.19 - fixing bug with parallel-random choice selection (during autotuning);
               found when working on OpenCL crowdtuning for BLAS ...
             - added possibility to use customized autotuner (directly changes pipeline)

* 2016.04.15 - moving platform.init entries to ck-env (as it is needed for environment, i.e. to support Windows)

* 2016.03.25 - adding support to extract MILEPOST/cTuning cc static program features (legacy)

* 2016.03.21 - adding universal OpenCL stubs package (1.2) targeting Android devices

* 2016.03.07 * adding support for pre processing scripts (improving OpenCL/CUDA/MPI autotuning)
             * adding extra tags to select datasets (useful for mobile phone crowdtuning to select small datasets)

* 2016.02.26 * adding preliminary support to crowdsource program optimization using Android mobile phones

* 2016.02.17 * adding dimension reduction to pipeline 
               (leaving only influential optimizations + inverting and turning off all if needed)
               very important for proper collaborative machine learning
             * changed platform.accelerator -> platform.gpu

* 2016.02.13 * improving checking of pre-existing solutions. Started implementing automatic pruning of choices ...

* 2016.02.12 * adding support to check pre-selection solutions (found during crowdtuning)
               during autotuning (preparation for clustering and optimization reactions)
             * recording characteristics to pre-selected solutions

* 2016.02.10 * adding ARM and general CPU specific flags for GCC and LLVM
             * changing arch-specific to cpu-specific tags 

* 2016.02.09 * improving GCC compiler flag description
             * adding support to explore compiler flags with various tags (boolean, parametric, arch-specific)

* 2016.02.08 * preparing LLVM descriptions
             * fixing detection of the most close compiler description (for autotuning/crowdtuning)
             * adding aggregation of failed cases (to automatically/manually detect compiler bugs)

* 2016.02.05 * started adding pipeline key descriptions (user-friendly info)
             * various small fixes and enhancements for experiment replay

* 2016.02.04 * improving search for the most close compiler description (version)
             * adding GCC v5.3 and v6.0 descriptions

* 2016.02.03 * adding detected compiler description to choices
             * improving compiler optimization flag web viewer

* 2016.02.01 * added mode to skip recording pipeline/desc info to experiment 
               (to avoid sending huge files between servers during optimization crowdsourcing)

* 2016.01.29 * general improvements
             * adding calculation of multi-dimensional improvements
             * adding universal conditions on solutions during auto-tuning
               (such as improve energy without sacrificing execution time,
                or improve execution time without increasing code size ...)

* 2016.01.15 * returning added or deleted points/sub-points during autotuning
               (for crowd-tuning)
             * Adding margins when comparing values on Pareto frontier 
               (used to monitor only significant speedups for when crowdsourcing autotuning
                particularly across mobile devices)
             * Fixing small bug in RTL for older MILEPOST project extracted codelets
               (to crowd-tune them and them use to speed up optimization and modeling
                large apps based on these codelets - see our papers)

* 2016.01.11 * adding support to select program, cmd, dataset and dataset file randomly
               to support program multi-objective crowdtuning
             * adding 'ck clean_tmp program:*' to clean tmp dirs in all program entries
               (to clean up results of crowd-tuning)

* 2016.01.04 * adding crowdtune action -> redirecting to "ck crowdsource program.optimization"

* 2015.12.31 * adding dummy "program.output" to check program output during benchmarking/tuning

* 2015.12.30 * adding "ask_enter_after_each_iteration" - if 'yes', ask to press Enter after each iteration
               - useful for debugging, analyzing Pareto and checking speedups
             * fixing bug with feature selection for Pareto

* 2015.12.15 * adding "force_pipeline_update" key to properly update pipelines (workflows) during replay

* 2015.12.13 * adding boolean keys "compilation_success_bool" and "run_success_bool" (True/False)
               to autotuning pipeline to speed up predictive analytics
             * recording extra info for reproducibility in flat array (pipeline_state 
               with statistical repetition number and with pipeline fail and reason)

* 2015.12.12 * failing autotuning pipeline if post-processing scripts fails

* 2015.12.07 * fixing compiler flag autotuning demo (small API change)
             * fixing a bug in scatter graphs (found when plotting results of multi-objective optimization)

* 2015.12.06 * adding 'local_platform' key to be able to use local platform parameters
               (useful to retarget experiments from another machine)

* 2015.11.23 * adding algorithm module dummy (future work)

* 2015.11.22 * fixing compile flag autotuning demos; 
             * fixing a few bugs in random compiler flag autotuning (iterative compilation);

* 2015.11.20 * fixing problem with return code when compiling programs, if programs are not from GIT repo

* 2015.11.16 * adding correct selection of default parameters for the first iteration of random autotuning 
               (noticed during dgemmbench OpenCL autotuning)

* 2015.11.10 * adding "sleep" parameter to autotune pipeline (set delay between iterations)
             * fixing error in exhaustive exploration when using "loop" with "choices" 
             * adding extra_dataset_files to dataset module to support more files
               when sending to Android (such as OpenCL autotuning via mobile phones)

* 2015.10.28 * adding CK_PROG_COMPILER_VARS when compiling via script ...
             * adding "git rev-parse HEAD" during compilation to check
               GIT hash for the compiled program (useful to debug and explain
                unexpected behavior when reproducing experiments)

* 2015.10.27 * adding 'cp' to copy programs and update 'backup_data_uid' (fixing issue #41)

* 2015.10.22 * adding support to compile via scripts 
                    (useful for PENCILCC from CARP project and LLVM LNT)
                  * adding "add_extra_env_for_compilation" in compilation meta
                    (useful for PENCILBENCH)

* 2015.09.25 * fixing xopenme.h for Windows 

* 2015.09.22 * fixing pipeline update when dataset is changed (i.e. updating related dataset files)
                    (reported by Anton)
                  * taking statistical repeat from pipeline if exists (to properly replay experiments) 

* 2015.09.10 * moved 'script:platform.init' to 'platform.init:*'

* 2015.09.09 * adding 'skip_stat_analysis' to compact recorded data when stat analysis is not needed
                    (useful for CLSmith)
                  * adding function 'import_all_files' in 'dataset' to automatically register all dataset files

* 2015.09.08 * improving dataset file selector (suggested by Anton)
                  * making text to select UIDs less ambiguous (asked by Anton)
                  * fixing program calibration problem for slambench (detected by Anton)
                  * if iterations in pipeline == -1, infinite loop (suggested by Andrei Lascu)

* 2015.09.02 * turning off 'state check' (frequency change during experiments) by default
                    in autotuning demos

* 2015.09.01 * adding timeout for program compilation and execution
                    (useful for research in computer engineering where
                    generated code may be buggy and result in infinite loops,
                    i.e. during autotuning or compiler testing and bug detection)
                  * providing more info about compilation and execution failures
                  * adding possibility to record only failed cases
                    (useful to crowdsource testing compilers/programs/os/architecture for bugs)

* 2015.08.12 * adding 'auto' tag to identify/detect automatically extracted compiler descriptions
                  * adding parsing of reserved key $#ck_take_from_{CID}$# - for example, used to calculate accuracy for autotuning in slambench
                  * adding possibility to reverse key order in Pareto frontier filter during multi-objective autotuning 
                    (useful for FPS in slambench, for example)

* 2015.08.11 * showing compiler flags in HTML (when viewing entries)
                  * fixing some wrong compiler flag descriptions

* 2015.08.10 * added GCC 5.2.0 CK package (will be downloaded and installed)

* 2015.08.09 * add extra dict when recording experiments (useful to record name and subview_uoa, for example)
                  * fixing a few bugs in experiment viewing
